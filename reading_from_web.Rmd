---
title: "Reading Data From the Web"
author: "Lindsey Covell"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(rvest)
library(httr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## NSDUH data: extracting tables 

```{r}
url = "http://samhda.s3-us-gov-west-1.amazonaws.com/s3fs-public/field-uploads/2k15StateFiles/NSDUHsaeShortTermCHG2015.htm"
drug_use_html = read_html(url)
```

Picking out the first table 

```{r}
drug_use_html %>%
  html_table() %>% 
  first() %>% 
  slice(-1)
```

## SW Data w/ CSS Selectors 

```{r}
swm_html = 
  read_html("https://www.imdb.com/list/ls070150896/")
```

How to get the stuff you want use Selector Gadget 

```{r}
sw_titles = 
  swm_html %>%
  html_elements(".lister-item-header a") %>%  ## find CSS tag w/ Sel Gadg
  html_text()    #Converts all to text

sw_runtime = 
  swm_html %>%
  html_elements(".runtime") %>%  
  html_text()

sw_money = 
  swm_html %>%
  html_elements(".text-small:nth-child(7) span:nth-child(5)") %>%
  html_text()

### Mesh all of these together and make a df ###

swm_df = 
  tibble(
    title = sw_titles,
    rev = sw_money,
    runtime = sw_runtime)

```

## Using an API 

```{r}
water_df = 
  GET("https://data.cityofnewyork.us/resource/ia2d-e54m.csv") %>% 
  content("parsed")
```

```{r}
brfss_smart2010 = 
  GET("https://chronicdata.cdc.gov/resource/acme-vg9e.csv",
      query = list("$limit" = 5000)) %>% 
  content("parsed")
```

Pokemon 

```{r}
poke = 
  GET("http://pokeapi.co/api/v2/pokemon/1") %>%
  content()

names(poke)  #get all names 

poke[["species"]]  #get vaule of the specfic name 
```

